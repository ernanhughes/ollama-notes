{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/ernanhughes/ollama-notes/blob/main/notebooks/rag.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymupdf\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    text = ''\n",
    "    with pymupdf.open(pdf_path) as pdf:\n",
    "        for page in pdf:\n",
    "            text += page.get_text()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69078\n"
     ]
    }
   ],
   "source": [
    "text = extract_text_from_pdf(\"2005.11401v4.pdf\")\n",
    "print(len(text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text, chunk_size, overlap=0):\n",
    "  \"\"\"\n",
    "  Splits text into chunks of specified length, ensuring words are not split.\n",
    "\n",
    "  Args:\n",
    "    text: The text to split.\n",
    "    chunk_size: The desired size of each chunk.\n",
    "    overlap: The number of characters to overlap between chunks. Defaults to 0.\n",
    "\n",
    "  Returns:\n",
    "    A list of text chunks.\n",
    "  \"\"\"\n",
    "\n",
    "  chunks = []\n",
    "  start = 0\n",
    "  while start < len(text):\n",
    "    # Find the next word boundary within the chunk size\n",
    "    end = start + chunk_size\n",
    "    if end >= len(text):\n",
    "      end = len(text)\n",
    "    else:\n",
    "      while end > start and text[end] != \" \" and end > start + overlap:\n",
    "        end -= 1\n",
    "\n",
    "    chunks.append(text[start:end])\n",
    "    start = end + overlap\n",
    "\n",
    "  return chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61\n"
     ]
    }
   ],
   "source": [
    "chunks = chunk_text(text, chunk_size=1024, overlap=128)\n",
    "ids = []\n",
    "for i, chunk in enumerate(chunks):\n",
    "    ids.append(f\"chunk_{i}\")\n",
    "\n",
    "print(len(chunks))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "OLLAMA_EMBEDDING_MODEL=\"mxbai-embed-large\"\n",
    "OLLAMA_BASE_URL='http://127.0.0.1:11434'\n",
    "\n",
    "def generate_embeddings(text, model_name: str = OLLAMA_EMBEDDING_MODEL,\n",
    "                        base_url: str = OLLAMA_BASE_URL):\n",
    "    \"\"\"Generate embeddings for the given text using the specified model.\"\"\"\n",
    "    try:\n",
    "        url = f\"{base_url}/api/embeddings\"\n",
    "        data = {\n",
    "            \"prompt\": text,\n",
    "            \"model\": model_name\n",
    "        }\n",
    "        response = requests.post(url, json=data)\n",
    "        if response.status_code == 200:\n",
    "            return response.json()[\"embedding\"]\n",
    "        else:\n",
    "            print(f\"Failed to generate embeddings. Status code: {response.status_code}\")\n",
    "            print(\"Response:\", response.text)\n",
    "            return None\n",
    "    except requests.ConnectionError:\n",
    "        print(\"Failed to connect to the Ollama server. Make sure it is running locally and the URL is correct.\")\n",
    "        return None\n",
    "    except json.JSONDecodeError:\n",
    "        print(\"Failed to parse JSON response from Ollama server.\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('v0.1.3',)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sqlite3 import connect\n",
    "import sqlite_vec\n",
    "db_file = \"rag.db\"\n",
    "cn =connect(db_file)\n",
    "cur = cn.cursor()\n",
    "cn.enable_load_extension(True)\n",
    "sqlite_vec.load(cn)\n",
    "cn.enable_load_extension(False)\n",
    "ver = cur.execute(\"select vec_version()\").fetchone()\n",
    "ver\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of embeddings: 1024\n"
     ]
    }
   ],
   "source": [
    "\n",
    "embeddings_len = 1024\n",
    "print(f\"Length of embeddings: {embeddings_len}\")\n",
    "create_table = f\"\"\"\n",
    "DROP TABLE IF EXISTS DOCUMENT_FTS;\n",
    "CREATE VIRTUAL TABLE DOCUMENT_FTS USING fts5(id UNINDEXED, content, tokenize=\"porter unicode61\");\n",
    "\n",
    "DROP TABLE IF EXISTS DOCUMENT_VECTOR;\n",
    "CREATE VIRTUAL TABLE DOCUMENT_VECTOR \n",
    "USING vec0(id INTEGER PRIMARY KEY, embedding float[{embeddings_len}]);\n",
    "\n",
    "DROP TABLE IF EXISTS DOCUMENT_LOOKUP;\n",
    "CREATE TABLE DOCUMENT_LOOKUP (id INTEGER PRIMARY KEY, content TEXT);\n",
    "\"\"\"\n",
    "cur.executescript(create_table)\n",
    "cn.commit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def serialize_f32(vec):\n",
    "    return np.array(vec, dtype=np.float32).tobytes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 61/61 [00:02<00:00, 21.77it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "cur.execute(\"DELETE FROM DOCUMENT_FTS;\")\n",
    "cur.execute(\"DELETE FROM DOCUMENT_VECTOR;\")\n",
    "cur.execute(\"DELETE FROM DOCUMENT_LOOKUP;\")\n",
    "\n",
    "i = 0\n",
    "for chunk in tqdm(chunks):\n",
    "    i += 1\n",
    "    embedding = generate_embeddings(chunk)\n",
    "    cur.execute(\"INSERT INTO DOCUMENT_FTS(id, content) VALUES (?, ?)\", (i, chunk))\n",
    "    cur.execute(\"INSERT INTO DOCUMENT_VECTOR(id, embedding) VALUES (?, ?)\", (i, serialize_f32(embedding)))\n",
    "    cur.execute(\"INSERT INTO DOCUMENT_LOOKUP(id, content) VALUES (?, ?)\", (i, chunk))\n",
    "cn.commit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reciprocal_rank_fusion(fts_results, vec_results, k=60):  \n",
    "    rank_dict = {}  \n",
    "  \n",
    "    # Process FTS results  \n",
    "    for rank, (id,) in enumerate(fts_results):  \n",
    "        if id not in rank_dict:  \n",
    "            rank_dict[id] = 0  \n",
    "        rank_dict[id] += 1 / (k + rank + 1)  \n",
    "  \n",
    "    # Process vector results  \n",
    "    for rank, (rowid, distance) in enumerate(vec_results):  \n",
    "        if rowid not in rank_dict:  \n",
    "            rank_dict[rowid] = 0  \n",
    "        rank_dict[rowid] += 1 / (k + rank + 1)  \n",
    "  \n",
    "    # Sort by RRF score  \n",
    "    sorted_results = sorted(rank_dict.items(), key=lambda x: x[1], reverse=True)  \n",
    "    return sorted_results \n",
    "  \n",
    "def or_words(input_string):  \n",
    "    # Split the input string into words  \n",
    "    words = input_string.split()  \n",
    "      \n",
    "    # Join the words with ' OR ' in between  \n",
    "    result = ' OR '.join(words)  \n",
    "      \n",
    "    return result\n",
    "\n",
    "def lookup_row(id):\n",
    "    row_lookup = cur.execute('''  \n",
    "    SELECT content FROM DOCUMENT_LOOKUP WHERE id = ?\n",
    "    ''', (id,)).fetchall()  \n",
    "    content = ''\n",
    "    for row in row_lookup:\n",
    "        content= row[0]\n",
    "        break\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| RRF Score: 0.032266458495966696 |ID: 23| Content: otland is Pound sterling.\n",
      "RAG-T Pound is the currency needed in Scotland.\n",
      "RAG-S The currency needed in Scotland is the pound sterling.\n",
      "Jeopardy\n",
      "Question\n",
      "Gener\n",
      "-ation\n",
      "Washington\n",
      "BART\n",
      "?This state has the largest number of counties in the U.S.\n",
      "RAG-T It’s the only U.S. state named for a U.S. president\n",
      "RAG-S It’s the state where you’ll ﬁnd Mount Rainier National Park\n",
      "The Divine\n",
      "Comedy\n",
      "BART\n",
      "*This epic poem by Dante is divided into 3 parts: the Inferno, the Purgatorio & the Purgatorio\n",
      "RAG-T Dante’s \"Inferno\" is the ﬁrst part of this epic poem\n",
      "RAG-S This 14th century work is divided into 3 sections: \"Inferno\", \"Purgatorio\" & \"Paradiso\"\n",
      "For 2-way classiﬁcation, we compare against Thorne and Vlachos [57], who train RoBERTa [35]\n",
      "to classify the claim as true or false given the gold evidence sentence. RAG achieves an accuracy\n",
      "within 2.7% of this model, despite being supplied with only the claim and retrieving its own evidence.\n",
      "We also analyze whether documents retrieved by RAG correspond to documents annotated as\n",
      "| RRF Score: 0.01639344262295082 |ID: 27| Content: r 10 retrieved latent\n",
      "documents, and we do not observe signiﬁcant differences in performance between them. We have the\n",
      "ﬂexibility to adjust the number of retrieved documents at test time, which can affect performance and\n",
      "runtime. Figure 3 (left) shows that retrieving more documents at test time monotonically improves\n",
      "Open-domain QA results for RAG-Sequence, but performance peaks for RAG-Token at 10 retrieved\n",
      "documents. Figure 3 (right) shows that retrieving more documents leads to higher Rouge-L for\n",
      "RAG-Token at the expense of Bleu-1, but the effect is less pronounced for RAG-Sequence.\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "K Retrieved Docs\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "NQ Exact Match\n",
      "RAG-Tok\n",
      "RAG-Seq\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "K Retrieved Docs\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "NQ Answer Recall @ K\n",
      "RAG-Tok\n",
      "RAG-Seq\n",
      "Fixed DPR\n",
      "BM25\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "K Retrieved Docs\n",
      "48\n",
      "50\n",
      "52\n",
      "54\n",
      "56\n",
      "Bleu-1 / Rouge-L score\n",
      "RAG-Tok R-L\n",
      "RAG-Tok B-1\n",
      "RAG-Seq R-L\n",
      "RAG-Seq B-1\n",
      "Figure 3: Left: NQ performance as more documents are retrieved. Center: Retrieval recall perfor-\n",
      "mance in NQ. Right: MS-MARCO Bleu-1\n",
      "| RRF Score: 0.016129032258064516 |ID: 26| Content: \n",
      "77.8%\n",
      "46.8%\n",
      "RAG-Seq.\n",
      "83.5%\n",
      "53.8%\n",
      "Table 6: Ablations on the dev set. As FEVER is a classiﬁcation task, both RAG models are equivalent.\n",
      "Model\n",
      "NQ\n",
      "TQA\n",
      "WQ\n",
      "CT\n",
      "Jeopardy-QGen\n",
      "MSMarco\n",
      "FVR-3\n",
      "FVR-2\n",
      "Exact Match\n",
      "B-1\n",
      "QB-1\n",
      "R-L\n",
      "B-1\n",
      "Label Accuracy\n",
      "RAG-Token-BM25\n",
      "29.7\n",
      "41.5\n",
      "32.1\n",
      "33.1\n",
      "17.5\n",
      "22.3\n",
      "55.5\n",
      "48.4\n",
      "75.1\n",
      "91.6\n",
      "RAG-Sequence-BM25\n",
      "31.8\n",
      "44.1\n",
      "36.6\n",
      "33.8\n",
      "11.1\n",
      "19.5\n",
      "56.5\n",
      "46.9\n",
      "RAG-Token-Frozen\n",
      "37.8\n",
      "50.1\n",
      "37.1\n",
      "51.1\n",
      "16.7\n",
      "21.7\n",
      "55.9\n",
      "49.4\n",
      "72.9\n",
      "89.4\n",
      "RAG-Sequence-Frozen\n",
      "41.2\n",
      "52.1\n",
      "41.8\n",
      "52.6\n",
      "11.8\n",
      "19.6\n",
      "56.7\n",
      "47.3\n",
      "RAG-Token\n",
      "43.5\n",
      "54.8\n",
      "46.5\n",
      "51.9\n",
      "17.9\n",
      "22.6\n",
      "56.2\n",
      "49.4\n",
      "74.5\n",
      "90.6\n",
      "RAG-Sequence\n",
      "44.0\n",
      "55.8\n",
      "44.9\n",
      "53.4\n",
      "15.3\n",
      "21.5\n",
      "57.2\n",
      "47.5\n",
      "between these dates and use a template “Who is {position}?” (e.g. “Who is the President of Peru?”)\n",
      "to query our NQ RAG model with each index. RAG answers 70% correctly using the 2016 index for\n",
      "2016 world leaders and 68% using the 2018 index for 2018 world leaders. Accuracy with mismatched\n",
      "indices is low (12% with the 2018 index and 2016 leaders, 4% with the 2016 index and 2018 leaders).\n",
      "This shows we can update RAG’s world\n",
      "| RRF Score: 0.016129032258064516 |ID: 19| Content: 5\n",
      "72.5\n",
      "89.5\n",
      "RAG-Seq. 14.7\n",
      "21.4\n",
      "40.8\n",
      "44.2\n",
      "to more effective marginalization over documents. Furthermore, RAG can generate correct answers\n",
      "even when the correct answer is not in any retrieved document, achieving 11.8% accuracy in such\n",
      "cases for NQ, where an extractive model would score 0%.\n",
      "4.2\n",
      "Abstractive Question Answering\n",
      "As shown in Table 2, RAG-Sequence outperforms BART on Open MS-MARCO NLG by 2.6 Bleu\n",
      "points and 2.6 Rouge-L points. RAG approaches state-of-the-art model performance, which is\n",
      "impressive given that (i) those models access gold passages with speciﬁc information required to\n",
      "generate the reference answer , (ii) many questions are unanswerable without the gold passages, and\n",
      "(iii) not all questions are answerable from Wikipedia alone. Table 3 shows some generated answers\n",
      "from our models. Qualitatively, we ﬁnd that RAG models hallucinate less and generate factually\n",
      "correct text more often than BART. Later, we also show that RAG generations are more diverse than\n",
      "BART generations (see\n",
      "| RRF Score: 0.015873015873015872 |ID: 59| Content: -MARCO, where useful retrieved documents\n",
      "cannot always be retrieved, we observe that the model learns to always retrieve a particular set of\n",
      "documents for questions that are less likely to beneﬁt from retrieval, suggesting that null document\n",
      "mechanisms may not be necessary for RAG.\n",
      "G\n",
      "Parameters\n",
      "Our RAG models contain the trainable parameters for the BERT-base query and document encoder of\n",
      "DPR, with 110M parameters each (although we do not train the document encoder ourselves) and\n",
      "406M trainable parameters from BART-large, 406M parameters, making a total of 626M trainable\n",
      "18\n",
      "Table 7: Number of instances in the datasets used. *A hidden subset of this data is used for evaluation\n",
      "Task\n",
      "Train\n",
      "Development\n",
      "Test\n",
      "Natural Questions\n",
      "79169\n",
      "8758\n",
      "3611\n",
      "TriviaQA\n",
      "78786\n",
      "8838\n",
      "11314\n",
      "WebQuestions\n",
      "3418\n",
      "362\n",
      "2033\n",
      "CuratedTrec\n",
      "635\n",
      "134\n",
      "635\n",
      "Jeopardy Question Generation\n",
      "97392\n",
      "13714\n",
      "26849\n",
      "MS-MARCO\n",
      "153726\n",
      "12468\n",
      "101093*\n",
      "FEVER-3-way\n",
      "145450\n",
      "10000\n",
      "10000\n",
      "FEVER-2-way\n",
      "96966\n",
      "6666\n",
      "6666\n",
      "parameters. The best performing \"closed-book\" (parametric only)\n",
      "| RRF Score: 0.015625 |ID: 20| Content: eration,\n",
      "with both models outperforming BART on Q-BLEU-1. 4 shows human evaluation results, over 452\n",
      "pairs of generations from BART and RAG-Token. Evaluators indicated that BART was more factual\n",
      "than RAG in only 7.1% of cases, while RAG was more factual in 42.7% of cases, and both RAG and\n",
      "BART were factual in a further 17% of cases, clearly demonstrating the effectiveness of RAG on\n",
      "the task over a state-of-the-art generation model. Evaluators also ﬁnd RAG generations to be more\n",
      "speciﬁc by a large margin. Table 3 shows typical generations from each model.\n",
      "Jeopardy questions often contain two separate pieces of information, and RAG-Token may perform\n",
      "best because it can generate responses that combine content from several documents. Figure 2 shows\n",
      "an example. When generating “Sun”, the posterior is high for document 2 which mentions “The\n",
      "Sun Also Rises”. Similarly, document 1 dominates the posterior when “A Farewell to Arms” is\n",
      "generated. Intriguingly, after the ﬁrst token of each book is generated, the document\n",
      "| RRF Score: 0.015625 |ID: 11| Content: ce, and keep the document encoder (and\n",
      "index) ﬁxed, only ﬁne-tuning the query encoder BERTq and the BART generator.\n",
      "2.5\n",
      "Decoding\n",
      "At test time, RAG-Sequence and RAG-Token require different ways to approximate arg maxy p(y|x).\n",
      "RAG-Token\n",
      "The RAG-Token model can be seen as a standard, autoregressive seq2seq genera-\n",
      "tor with transition probability: p′\n",
      "θ(yi|x, y1:i−1) = P\n",
      "z∈top-k(p(·|x)) pη(zi|x)pθ(yi|x, zi, y1:i−1) To\n",
      "decode, we can plug p′\n",
      "θ(yi|x, y1:i−1) into a standard beam decoder.\n",
      "RAG-Sequence\n",
      "For RAG-Sequence, the likelihood p(y|x) does not break into a conventional per-\n",
      "token likelihood, hence we cannot solve it with a single beam search. Instead, we run beam search for\n",
      "each document z, scoring each hypothesis using pθ(yi|x, z, y1:i−1). This yields a set of hypotheses\n",
      "Y , some of which may not have appeared in the beams of all documents. To estimate the probability\n",
      "of an hypothesis y we run an additional forward pass for each document z for which y does not\n",
      "appear in the beam, multiply generator probability\n",
      "| RRF Score: 0.015384615384615385 |ID: 17| Content: ion signals aren’t available, and\n",
      "models that do not require such supervision will be applicable to a wider range of tasks. We explore\n",
      "two variants: the standard 3-way classiﬁcation task (supports/refutes/not enough info) and the 2-way\n",
      "(supports/refutes) task studied in Thorne and Vlachos [57]. In both cases we report label accuracy.\n",
      "4\n",
      "Results\n",
      "4.1\n",
      "Open-domain Question Answering\n",
      "Table 1 shows results for RAG along with state-of-the-art models. On all four open-domain QA\n",
      "tasks, RAG sets a new state of the art (only on the T5-comparable split for TQA). RAG combines\n",
      "the generation ﬂexibility of the “closed-book” (parametric only) approaches and the performance of\n",
      "\"open-book\" retrieval-based approaches. Unlike REALM and T5+SSM, RAG enjoys strong results\n",
      "without expensive, specialized “salient span masking” pre-training [20]. It is worth noting that RAG’s\n",
      "retriever is initialized using DPR’s retriever, which uses retrieval supervision on Natural Questions\n",
      "and TriviaQA. RAG compares favourably to the DPR QA system,\n",
      "| RRF Score: 0.015384615384615385 |ID: 8| Content: g, we formally introduce both models and then describe the\n",
      "pη and pθ components, as well as the training and decoding procedure.\n",
      "2.1\n",
      "Models\n",
      "RAG-Sequence Model\n",
      "The RAG-Sequence model uses the same retrieved document to generate\n",
      "the complete sequence. Technically, it treats the retrieved document as a single latent variable that\n",
      "is marginalized to get the seq2seq probability p(y|x) via a top-K approximation. Concretely, the\n",
      "top K documents are retrieved using the retriever, and the generator produces the output sequence\n",
      "probability for each document, which are then marginalized,\n",
      "pRAG-Sequence(y|x) ≈\n",
      "X\n",
      "z∈top-k(p(·|x))\n",
      "pη(z|x)pθ(y|x, z) =\n",
      "X\n",
      "z∈top-k(p(·|x))\n",
      "pη(z|x)\n",
      "N\n",
      "Y\n",
      "i\n",
      "pθ(yi|x, z, y1:i−1)\n",
      "RAG-Token Model\n",
      "In the RAG-Token model we can draw a different latent document for each\n",
      "target token and marginalize accordingly. This allows the generator to choose content from several\n",
      "documents when producing an answer. Concretely, the top K documents are retrieved using the\n",
      "retriever, and then the generator produces a\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "fts_search_query = \"RAG\" \n",
    "top_k = 5 \n",
    "\n",
    "fts_results = cur.execute('''  \n",
    "  SELECT id FROM DOCUMENT_FTS WHERE DOCUMENT_FTS MATCH ? \n",
    "  ORDER BY rank limit 5  \n",
    "''', (or_words(fts_search_query),)).fetchall()  \n",
    "  \n",
    "# Vector search query  \n",
    "query_embedding = generate_embeddings(fts_search_query)  \n",
    "vec_results = cur.execute('''  \n",
    "    SELECT id, distance FROM DOCUMENT_VECTOR \n",
    "    WHERE embedding MATCH ? and K = ?  \n",
    "    ORDER BY distance  \n",
    "''', [serialize_f32(query_embedding), top_k]).fetchall()  \n",
    "  \n",
    "# Combine results using RRF  \n",
    "combined_results = reciprocal_rank_fusion(fts_results, vec_results)  \n",
    "  \n",
    "df = pd.DataFrame(combined_results, columns=['id', 'score'])\n",
    "df\n",
    "# Print combined results  \n",
    "# for id, score in combined_results:  \n",
    "#     print(f'| RRF Score: {score} |ID: {id}| Content: {lookup_row(id)}')  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
